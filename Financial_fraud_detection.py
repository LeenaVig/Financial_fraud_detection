# -*- coding: utf-8 -*-
"""Copy of AI CAPSTONE PROJECT 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SDWX45xFy9YbJafphOV-ZSx_gxxmu9es
"""

import pandas as pd
import numpy as np
import seaborn as sns
from  matplotlib import pyplot as plt

from google.colab import drive
drive.mount('/content/drive')



data_train=pd.read_csv ("/content/drive/MyDrive/AI CAPSTONE PROJECT/Financial/train_data.csv")

data_train.head()

data_test=pd.read_csv("/content/drive/MyDrive/AI CAPSTONE PROJECT/Financial/test_data.csv")

data_test_hidden=pd.read_csv("/content/drive/MyDrive/AI CAPSTONE PROJECT/Financial/test_data_hidden.csv")

data_test.columns,data_test_hidden.columns

data_test.head()

data_test_hidden.head()

data_train.eq(0).sum(),data_test.eq(0).sum()

"""**Checking the shape of the three datasets provided**"""

data_train.shape,data_test.shape,data_test_hidden.shape

data_train.corr()

""" **Creating separate dataframes for fraudulent and non-fradulent records**"""

data_fraud=data_train.loc[data_train['Class']==1]
data_normal=data_train.loc[data_train['Class']==0]

print("Total number of fraudulent transactions are", len(data_fraud))
print("Total number of non fradulent TRANSCATIONS are", len(data_normal))

"""**Storing the number of fraudulent and non fraudulent transactions in variable called count_class**"""

count_class = data_train.Class.value_counts()
count_class

"""**Visualise the count of normal and fraud transactions**"""

plt.title('Transaction class distribution')
LABELS =['Normal','Fraud']
plt.xlabel('Class')
plt.xticks(range(2),LABELS)
plt.ylabel('Frequency')
count_class.plot(kind='bar')

"""**Visualizing the Amount per transaction in both fraudulent and non fraudulent transactions**"""

data_fraud.Amount.describe()

data_normal.Amount.describe()

fraud1,(ax1, ax2) = plt.subplots(2,1, sharex=True)
fraud1.suptitle("Amount per transaction by class")

bins = 8
ax1.hist(data_fraud.Amount, bins=bins)
ax2.hist(data_normal.Amount, bins = bins)
ax1.set_title("Fraud")
ax2.set_title("Normal")

plt.xlabel("Amount $")
plt.ylabel("Number of transactions")
plt.xlim(0,3000)
plt.yscale = ('log')
plt.show()

"""**HANDLING IMBALANCED DATASET**"""

!pip install imblearn

"""**Logistic Regression model on unbalanced data**"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,f1_score,roc_auc_score

model_log = LogisticRegression()

X= data_train.drop('Class',axis =1)
Y = data_train['Class']
ytest = data_test_hidden['Class']

X_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size = 0.2,random_state = 2,stratify = Y)
model_log.fit(X_train,Y_train)

#prediction on train data (unbalanced)
Y_train_prediction = model_log.predict(X_train)
print(classification_report(Y_train,Y_train_prediction))

# prediction on test data (fitting done on unbalanced data)
Y_test_prediction = model_log.predict(data_test)
print(classification_report(ytest,Y_test_prediction))

"""**Undersampling the data**"""

data_normal_under = data_normal.sample(count_class[1])

data_normal_under.shape

data_under = pd.concat([data_normal_under,data_fraud],axis = 0)

data_under.Class.value_counts()

data_under.shape

x = data_under.drop('Class',axis =1)
y = data_under['Class']

# Splitting the undersampled data into train and test set
Xtrain,Xval,Ytrain,Yval = train_test_split(x,y,test_size = 0.2,random_state = 42,stratify = y)

Ytrain.value_counts()

"""**Applying Logistic Regression Model to the data after undersampling**"""

model1_log = LogisticRegression()
model1_log.fit(Xtrain,Ytrain)

# model Evaluation

# Report for logistic regression (after undersampling) on train data
yunder_log_pred = model1_log.predict(Xtrain)
#printytest_under_pred)
print("ROC_AUC-SCORE obtained after logistic regression modeling on the undersampled train data is",roc_auc_score(Ytrain,yunder_log_pred))
print(classification_report(Ytrain,yunder_log_pred))
print("F1 SCORE obtained after logistic regression modeling on the undersampled train data is", f1_score(Ytrain,yunder_log_pred))

# Report for logistic regression (after undersampling) on test data

ytest_under_pred = model1_log.predict(data_test)

print(classification_report(ytest,ytest_under_pred))
print("ROC_AUC-SCORE obtained after logistic regression modeling on the test data is", roc_auc_score(ytest,ytest_under_pred))
print("F1 SCORE obtained after logistic regression modeling on the test data is", f1_score(ytest,ytest_under_pred))

"""**Oversampling the data**"""

data_fraud_over = data_fraud.sample(count_class[0],replace = True)

data_over = pd.concat([data_fraud_over,data_normal],axis = 0)

data_over.shape

data_over.Class.value_counts()

x1 = data_over.drop("Class",axis = 1)
y1 = data_over['Class']

# Splitting the oversampled data into train and test set
X1train,X1val,Y1train,Y1val = train_test_split(x1,y1,test_size = 0.2,random_state = 42,stratify = y1)

"""**Logistic regression on oversampled data**"""

model1_log.fit(X1train,Y1train)
X1train_prediction = model1_log.predict(X1train)

print(X1train_prediction)
print(classification_report(Y1train,X1train_prediction))
print("the ROC_AUC score after applying logistic regression model on oversampled training data set is" , roc_auc_score(Y1train, X1train_prediction))
print("the F1 score after applying logistic regression model on oversampled  training data set is" , f1_score(Y1train,X1train_prediction))

"""***Logistic regression model evaluation on test data***"""

y_test = data_test_hidden['Class']

(model1_log.score(data_test, y_test))

#Report for logistic regression after  on validation data set
yval_log_pred = model1_log.predict(X1val)
print(classification_report(Y1val,yval_log_pred))
print("The ROC_AUC score after applying logistic regression model on validation data set is", roc_auc_score(Y1val,yval_log_pred))
print("The F1 score after applying logistic regression model on validation data set is",f1_score(Y1val,yval_log_pred))

# Report for logistic regression (after oversampling) on test data

ytest_overlog_pred = model1_log.predict(data_test)

print(classification_report(ytest,ytest_overlog_pred))
print("The ROC_AUC score after applying logistic regression model on test data set is", roc_auc_score(ytest,ytest_overlog_pred))
print("The F1 score after applying logistic regression model on test data set is", f1_score(ytest,ytest_overlog_pred))

"""**SVM Modeling on oversampled data**"""

from sklearn.svm import SVC

model_svm = SVC(kernel='linear')

model_svm.fit(X1train,Y1train)

ypred_svm_over = model_svm.predict(data_test)    
print(classification_report(ytest,ypred_svm_over))
print("The ROC_AUC score after applying svm model on test data set is", roc_auc_score(ytest,ypred_svm_over))
print("The F1 score after applying svm on test data set is", f1_score(ytest,ypred_svm_over))

"""**SVM Modelling on undersampled data**"""

model_svm.fit(Xtrain,Ytrain)

ypred_svm_over = model_svm.predict(data_test)
print(classification_report(ytest,ypred_svm_over))
print("The ROC_AUC score after applying svm model on test data set is", roc_auc_score(ytest,ypred_svm_over))
print("The F1 score after applying svm on test data set is", f1_score(ytest,ypred_svm_over))

"""**Naive Bayes Model on undersampled data**"""

from sklearn.naive_bayes import GaussianNB

model_nb = GaussianNB()

model_nb.fit(Xtrain,Ytrain)
ypred_nb_test_under = model_nb.predict(data_test)
print(classification_report(ytest,ypred_nb_test_under))
print("The ROC_AUC score after applying naive bayes model on test data set is", roc_auc_score(ytest,ypred_nb_test_under))
print("The F1 score after applying naive bayes on test data set is", f1_score(ytest,ypred_nb_test_under))

"""**Naive Bayes Model on oversampled data**




"""

model_nb.fit(X1train,Y1train)

ypred_nb_over = model_nb.predict(data_test)    
print(classification_report(ytest,ypred_nb_over))
print("The ROC_AUC score after applying naive bayes model on test data set is", roc_auc_score(ytest,ypred_nb_over))
print("The F1 score after applying naive bayes on test data set is", f1_score(ytest,ypred_nb_over))

!pip install imbalanced-Learn
import imblearn

"""**Oversampling by SMOTE**"""

from imblearn.over_sampling import SMOTE

smote = SMOTE()

X_sm,y_sm = smote.fit_resample(X_train,Y_train)

X_sm,y_sm = smote.fit_resample(X,Y)

y_sm.value_counts()

XS_train,XS_val,YS_train,YS_val =train_test_split(X_sm,y_sm,test_size=0.2,random_state=15,stratify=y_sm)

YS_train.value_counts()

YS_val.value_counts()

"""

```
# This is formatted as code
```

**Logistic regression model on test data after oversampling by SMOTE**"""

model_log.fit(XS_train,YS_train)

ytestsm_prediction = model_log.predict(data_test)
print(classification_report(ytest,ytestsm_prediction))
print("The ROC_AUC score after applying logistic regression model on test data set is", roc_auc_score(ytest,ytestsm_prediction))
print("The F1 score after applying logistic regression on test data set is", f1_score(ytest,ytestsm_prediction))

ytestsm_svm_pred = model_svm.predict(data_test)
print(classification_report(ytest,ytestsm_svm_pred))
print("The ROC_AUC score after applying svm model on test data set is", roc_auc_score(ytest,ytestsm_svm_pred))
print("The F1 score after applying svm on test data set is", f1_score(ytest,ytestsm_svm_pred))

model_nb.fit(XS_train,YS_train)

ytestsm_nb_pred = model_nb.predict(data_test)
print(classification_report(ytest,ytestsm_nb_pred))
print("The ROC_AUC score after applying naive bayes model on test data after SMOTE oversampling is", roc_auc_score(ytest,ytestsm_nb_pred))
print("The F1 score after applying naive bayes model on test data after SMOTE oversampling is", f1_score(ytest,ytestsm_nb_pred))

"""## **Applying random forest classifier**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
rf = RandomForestClassifier()
rf.fit(X_train,Y_train)

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [2,4]
# Minimum number of samples required to split a node
min_samples_split = [2, 5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2]
# Method of selecting samples for training each tree
bootstrap = [True, False]

#creating paramater grid

param_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
print(param_grid)

y_predrf_unbalanced = rf.predict(X_train)


print(classification_report(Y_train,y_predrf_unbalanced))

"""### ***Using Randomised Search***"""

rf_random = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X_train, Y_train)

"""***Printing best parameters***"""

rf_random.best_params_

"""***predicitng on Train set***"""

y_predrf= rf_random.predict(X_train)

print(classification_report(Y_train,y_predrf))

"""***predicting on validation set***"""

y_predrf= rf_random.predict(X_val)
print(classification_report(Y_val,y_predrf))

ytest_rf = data_test_hidden['Class']
print(ytest_rf)

"""***predicting on test dataset***"""

y_predrf= rf_random.predict(data_test)
print(classification_report(ytest_rf,y_predrf))
print("The ROC_AUC score after applying random forest(with hyperparameter tuning) on test data set is", roc_auc_score(ytest,y_predrf))
print("The F1 score after applying random forest(with hyperparameter tuning) on test data set is", f1_score(ytest,y_predrf))

"""**XGBOOST**"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error
X, y = X_train, Y_train
dmatrix = xgb.DMatrix(data=x, label=y)
params={'objective':'reg:squarederror'}
cv_results = xgb.cv(dtrain=dmatrix, params=params, nfold=10, metrics={'rmse'}, as_pandas=True, seed=20)
print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())

#XGboost with hyperparameter tuning (fitted)
params={ 'objective':'reg:squarederror',
         'max_depth': 6, 
         'colsample_bylevel':0.5,
         'learning_rate':0.01,
         'random_state':20}
cv_results = xgb.cv(dtrain=dmatrix, params=params, nfold=10, metrics={'rmse'}, as_pandas=True, seed=20, num_boost_round=1000)
print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())

from xgboost import XGBClassifier
xb = XGBClassifier()
xb.fit(X_train,Y_train)

"""***predicting using XGBoost model on the train data***"""

trainpredict = xb.predict(X_train)
print(classification_report(trainpredict, Y_train))

"""***predicting using XGBoost model on the test data***

"""

testpredict = xb.predict(data_test)
print(classification_report(ytest_rf,testpredict))
print("The ROC_AUC score after applying XGBoost Classifier is", roc_auc_score(ytest_rf,testpredict))
print("The F1 score after applying XGBoost Classifier is", f1_score(ytest_rf,testpredict))

"""**Building an ANN**
**(a) with only two layers**

"""

from sklearn.metrics import roc_auc_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Activation,Dense,BatchNormalization,Dropout
from tensorflow.keras.models import Sequential
#from tensorflow.keras.optimizers import optimis
from sklearn.metrics import confusion_matrix , classification_report
from sklearn.model_selection import GridSearchCV
import keras
from keras.wrappers.scikit_learn import KerasClassifier

def ann_model_2layers(xtrain, ytrain,xtest,ytest,neurons,b,epochs):
    # define model
    model = Sequential()
    # define first hidden layer and visible layer
    model.add(Dense(neurons, input_dim=30, activation='relu', kernel_initializer='he_uniform'))
    # define output layer
    model.add(Dense(1, activation='sigmoid'))
    # define loss and optimizer
    model.compile(loss='binary_crossentropy', optimizer='adam')
    model.fit(xtrain,ytrain,batch_size = b,epochs=epochs)
    y_pred=model.predict(xtest)
    y_pred = np.round(y_pred)
    score1 =  roc_auc_score(ytest, y_pred)
    print("The ROC_AUC Score with",neurons,"neurons in the first layer and batch size as ", b, "over",epochs,"epochs is ", score1)
    print(classification_report(ytest,y_pred))
    print(confusion_matrix(ytest,y_pred))

"""**Running the ANN model (2 Layers ) with 30 EPOCHS**"""

batch_size = 20
ann_model_2layers(X1train,Y1train,data_test,ytest,30,batch_size,30)

"""**Building an ANN (b) with two layers with a dropout layer**"""

def ann_model_2layersDropout(xtrain, ytrain,xtest,ytest,neurons1,b,epochs):
    # define model
    model = Sequential()
    # define first hidden layer 
    model.add(Dense(neurons1, input_dim=30, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.3))
    # define output layer
    model.add(Dense(1, activation='sigmoid'))
    # define loss and optimizer
    model.compile(loss='binary_crossentropy', optimizer='adam')
    model.fit(xtrain,ytrain,batch_size = b,epochs=epochs)
    y_pred=model.predict(xtest)
    y_pred = np.round(y_pred)
    score1 =  roc_auc_score(ytest, y_pred)
    print("The ROC_AUC Score with",neurons1,"neurons in the first layer and batch size as ", b, "over",epochs,"epochs is ", score1)
    print(classification_report(ytest,y_pred))
    print(confusion_matrix(ytest,y_pred))

batch_size = 20
ann_model_2layersDropout(X1train,Y1train,data_test,ytest,30,batch_size,30)

def define_model(activation = 'relu',init_weights = 'uniform',optimizer='adam'):
    # define model
    model = Sequential()
    # define first hidden layer and visible layer
    model.add(Dense(32, input_dim=30, activation=activation, kernel_initializer=init_weights))
    #adding dropout layer
    model.add(Dropout(0.1))
    model.add(Dense(32, activation=activation, kernel_initializer=init_weights))
    # define output layer
    model.add(Dense(1,kernel_initializer=init_weights, activation='sigmoid'))
    # define loss and optimizer
    model.compile(loss='binary_crossentropy',
                  optimizer=optimizer,
                  metrics =['accuracy'])

    return model
batch_size = 20
epochs = 2
model = KerasClassifier(build_fn=define_model,
                        epochs = epochs,
                        batch_size=batch_size,
                        verbose = 1)
activation = ['relu','sigmoid']
init_weights =['uniform']
optimizers = ['adam','SGD','RMSprop']
param_grid = dict(activation = activation,init_weights=init_weights,optimizer = optimizers)
grid = GridSearchCV(estimator=model, param_grid = param_grid)
grid_result = grid.fit(X1train,Y1train)
print("Best: % using %s" % (grid_result.best_score_,grid_result.best_params_))

ypred_ann = grid.predict(data_test)
print(classification_report(ytest,ypred_ann))
score_ann =  roc_auc_score(ytest, ypred_ann)
print("The ROC_AUC Score from the ANN after grid search is " , score_ann)

"""**APPLYING ANOMALY DETECTION ALGORITHMS**

***Outlier detection model***
"""

X.shape,Y.shape

import sklearn
from sklearn import svm
from sklearn.model_selection import train_test_split,cross_validate
from sklearn.metrics import confusion_matrix,classification_report,f1_score,accuracy_score
X= data_train.drop('Class',axis =1)
Y = data_train['Class']
ytest = data_test_hidden['Class']
Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size=0.2, random_state=2, stratify=Y)
#define outlier detection model
model_svm = svm.OneClassSVM(gamma='scale', nu=0.00172)
# fit on majority class

model_svm.fit(Xtrain,Ytrain)

X.shape,Y.shape

# detect outliers in the test set
yhat_svm = model_svm.predict(data_test)
print(yhat_svm)

data_test_hidden['Class']

YTEST = data_test_hidden['Class']
print(YTEST)

# mark inliers 1, outliers -1 for the test set (YTEST)
YTEST[YTEST==1] = -1
YTEST[YTEST == 0] = 1

YTEST

print(f1_score(YTEST,yhat_svm))

print(classification_report(YTEST,yhat_svm))

print(ytest.shape)

X_train.shape, Y_train.value_counts()

"""### ***Isolation Forest***"""

from sklearn.ensemble import IsolationForest

model_if = IsolationForest(contamination=0.00172,max_features = 10,random_state=32)

X.shape,Y.shape

x3train,x3val,y3train,y3val = train_test_split(X,Y,test_size=0.2,random_state=2,stratify = Y)

# fit on majority class

model_if.fit(X)

print(y3train[:100])

# mark inliers 1, outliers -1 for the test set
y3train[y3train == 1] = -1
y3train[y3train == 0] = 1

print(y3train[:100])

yhat_if_x3train = model_if.predict(x3train)

score_if_tr = f1_score(y3train, yhat_if_x3train, pos_label=-1)

print(score_if_tr)

"""**Predicting anomaly detection by isolation forest on the train data set**"""

data_test.shape

data_test.shape

print(classification_report(y3train, yhat_if_x3train))

"""**Predicting anomaly detection by isolation forest on the test data set**"""

yhat_if_test = model_if.predict(data_test)

print(classification_report(YTEST, yhat_if_test))

"""***Elliptic Envelope***"""

from sklearn.covariance import EllipticEnvelope
# EllipticEnvelope model
model_EE = EllipticEnvelope(random_state=32,assume_centered=True)
# fit on majority class

model_EE.fit(X)

# predict outliers in the train set
yhat_ee = model_EE.predict(x3train)
# calculate score
F1_score_ee_train = f1_score(y3train,yhat_ee, pos_label=-1)

print(classification_report(y3train,yhat_ee))

# predict outliers in the test set
yhat_ee_test = model_EE.predict(data_test)

print(YTEST)

# Anomaly detection using Elliptic Envelope 

print(classification_report(YTEST,yhat_ee_test))

"""***Local Outlier Factor***"""

# define outlier detection model
import numpy as np
from numpy import vstack
from sklearn.neighbors import LocalOutlierFactor
model_lof = LocalOutlierFactor(contamination=0.00172)

# make prediction on composite dataset
yhat_lof = model_lof.fit_predict(x3train)

print(classification_report(y3train,yhat_lof))
Score_LOF = f1_score(y3train,yhat_lof, pos_label=-1)
print('F1 Score_LOF: %.3f' % Score_LOF)

yhat_lof_test = model_lof.fit_predict(data_test)

YTEST[:100]

ytest1= data_test_hidden['Class']
ytest1[:100]

print(classification_report(YTEST,yhat_lof_test))
Score_LOF = f1_score(YTEST,yhat_lof_test, pos_label=-1)
print('F1 Score_LOF: %.3f' % Score_LOF)

"""**Guassian mixture model**"""

from sklearn.mixture import GaussianMixture

X_train.columns

X.columns

gm = GaussianMixture(n_components=2, random_state=0).fit(X_train)

gm.predict(data_test)

GM = gm.score_samples(data_test)

test_gm = data_test

gm.score_samples(X_train)

gm.score_samples(X_train).shape

gm.score_samples(X).shape

df2=pd.concat([X,Y],axis=1)
df2.shape

df2['guassian_score'] = gm.score_samples(X)

df2.shape

df2['guassian_score']

df2['guassian_score'].min(),df2['guassian_score'].max()

norm = df2[df2['Class']==0]

fraud1 =df2[df2['Class']==1]

norm['guassian_score'].min(),norm['guassian_score'].max()

fraud1['guassian_score'].min(),fraud1['guassian_score'].max()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
#plt.style.use('seaborn-whitegrid')

norm['guassian_score'].shape,fraud1['guassian_score'].shape

y =np.ones(227451)

y2=np.ones(394)

"""**Visualising the probability scores of fraudulent and non-fraudulent transactions**"""

plt.scatter(norm['guassian_score'],y)
plt.title("PROBABILITY SCORES FOR NORMAL TRANSACTIONS")
plt.xlabel('probability scores')

plt.scatter(fraud1['guassian_score'],y2,c='green')
plt.title("PROBABILITY SCORES FOR FRAUD TRANSACTIONS")
plt.xlabel('probability scores')

# set a threshold for markng a transcation as fraud or normal.
#setting threhold as 3500

# Running isolation forest again using the Guassian scores as an engineered feature

test_data_gm = data_test.copy()

test_data_gm['guassian_score'] = GM

df2.columns

Xtrain_gm = df2.drop('Class',axis = 1)
Ytrain_gm = df2['Class']
model_if.fit(Xtrain_gm)

ypred_gm = model_if.predict(test_data_gm )

print(classification_report(YTEST,ypred_gm))

model_EE.fit(Xtrain_gm)

ytest[:100]

ypred_gm_ee = model_EE.predict(test_data_gm )

print(classification_report(YTEST,ypred_gm_ee))

model_svm = svm.SVC()
model_svm.fit(Xtrain_gm,Ytrain_gm)

"""SVM model Evaluation
On taining data

**SVM model Evaluation On test data**
"""

# Accuracy on test data
X_test_predict_svm = model_svm.predict(test_data_gm)
accuracy_test_svm = accuracy_score(YTEST,X_test_predict_svm)

print(classification_report(YTEST,X_test_predict_svm))

print("Accuracy using SVM model on training data is ", accuracy_train_svm)

print("Accuracy using SVM model on test data is ", accuracy_test_svm)